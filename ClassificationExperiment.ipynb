{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_svmlight_file(\"a9a.txt\")\n",
    "X_train = data[0].todense()\n",
    "y_train = data[1]\n",
    "data = load_svmlight_file(\"a9a.t\")\n",
    "X_test = data[0].todense()\n",
    "y_test = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.c_[X_test, np.zeros(X_test.shape[0])]      #验证集少了一个属性 补全为0\n",
    "X_train = np.c_[X_train, np.ones(X_train.shape[0])]    #加一维 全为1\n",
    "X_test = np.c_[X_test, np.ones(X_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = 1\n",
    "iter = 1000                               #迭代次数\n",
    "dimension = X_train.shape[1]              #维数\n",
    "regular_param = 1                         #正则化参数\n",
    "batch_size = int(X_train.shape[0] / 325)      #batch大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#w = np.zeros(dimension)                      #全零初始化\n",
    "w = np.random.normal(size = (dimension))      #正态分布初始化\n",
    "G = np.zeros(dimension)                       #梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X_train, y_train):\n",
    "    #打乱矩阵 取batch\n",
    "    random_sequence = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(random_sequence)\n",
    "    X_batch = np.zeros((batch_size, dimension))\n",
    "    y_batch = np.zeros(batch_size)\n",
    "    for i in range(batch_size):\n",
    "        X_batch[i] = X_train[random_sequence[i], :]\n",
    "        y_batch[i] = y_train[random_sequence[i]]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient (X, y, w):\n",
    "    g = np.zeros(X.shape[1])   #存放 hinge loss 的梯度\n",
    "    for i in range(X.shape[0]):   #对每一条记录迭代一次\n",
    "        judge = y[i] * np.dot(X[i], w.reshape(dimension, 1))   #分段函数 判断\n",
    "        if (judge < 1):               #小于1 则更新g 否则 g = g + 0\n",
    "            g = g - y[i] * X[i]\n",
    "    G = w + C * g              #计算梯度G\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss (X, y, w):\n",
    "    loss = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        judge = y[i] * np.dot(X[i], w.reshape(dimension, 1))    #分段函数 判断y * X * W与1的大小\n",
    "        if (judge >= 1):\n",
    "            loss = loss + 0                                     #大于1 hinge loss = 0\n",
    "        else:\n",
    "            loss = loss + 1 - judge                             #小于1 加上 1 - y * X * W\n",
    "    loss = np.dot(w, w.reshape(dimension, 1)) / 2 + C * loss / X.shape[0]                #loss取平均\n",
    "    return loss                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NAG(w, v, g, eta=0.05, gama=0.9):\n",
    "    v = gama * v + eta * g\n",
    "    w = w - v\n",
    "    return w, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSProp(w, G, g, gama=0.9, eta=0.001, epsilon=1e-8):\n",
    "    G = gama * G + (1 - gama) * np.dot(g, g.reshape(g.shape[1], 1))\n",
    "    w = w - (eta / np.sqrt(G + epsilon)) * g\n",
    "    return w, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AdaDelta(w, G, g, delta, gama=0.95, epsilon=1e-6):\n",
    "    G = gama * G + (1-gama) * np.dot(g, g)\n",
    "    delta_w = -(np.sqrt(delta + epsilon) / np.sqrt(G + epsilon)) * g\n",
    "    w = w + delta_w\n",
    "    delta = gama * delta + (1-gama) * np.dot(delta_w, delta_w.reshape(delta_w.shape[0], 1))\n",
    "    return w, G, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Adam(w, m, G, g, t, beta=0.9, gama=0.999, eta=0.002, epsilon=1e-8):\n",
    "    m = beta * m + (1-beta) * g\n",
    "    G = gama * G + (1-gama) * np.dot(g, g.reshape(g.shape[0], 1))\n",
    "    a = eta * np.sqrt(1 - np.power(gama, t))\n",
    "    w = w - (a / np.sqrt(G + epsilon)) * m\n",
    "    t = t + 1\n",
    "    return w, m, G, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NAG_run(X_train, y_train, X_test, y_test, w):\n",
    "    gama = 0.9\n",
    "    v = np.zeros(dimension)\n",
    "    NAG_loss = np.zeros(iter)   #初始化 存放NAG法的验证集loss的向量\n",
    "    for i in range(iter):           #迭代\n",
    "        NAG_loss[i] = loss(X_test, y_test, w)\n",
    "        g = gradient(X_train, y_train, w - gama * v)\n",
    "        w, v = NAG(w, v, g)\n",
    "    return NAG_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSProp_run(X_train, y_train, X_test, y_test, w):\n",
    "    G = 0\n",
    "    RMSProp_loss = np.zeros(iter)   #初始化 存放NAG法的验证集loss的向量\n",
    "    for i in range(iter):\n",
    "        RMSProp_loss[i] = loss(X_test, y_test, w)\n",
    "        g = gradient(X_train, y_train, w)\n",
    "        w, G = RMSProp(w ,G, g)\n",
    "    return RMSProp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AdaDelta_run(X_train, y_train, X_test, y_test, w):\n",
    "    G = 0\n",
    "    delta = 0\n",
    "    AdaDelta_loss = np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        AdaDelta_loss[i] = loss(X_test, y_test, w)\n",
    "        g = gradient(X_train, y_train, w)\n",
    "        w, G, delta = AdaDelta(w, G, g, delta)\n",
    "    return AdaDelta_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Adam_run(X_train, y_train, X_test, y_test, w):\n",
    "    m = np.zeros(dimension)\n",
    "    G = 0\n",
    "    t = 0\n",
    "    Adam_loss = np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        Adam_loss[i] = loss(X_test, y_test, w)\n",
    "        g = gradient(X_train, y_train, w)\n",
    "        w, m, G, t = Adam(w, m, G, g, t)\n",
    "    return Adam_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NAG_loss = NAG_run(X_train, y_train, X_test, y_test, w)\n",
    "RMSProp_loss = RMSProp_run(X_train, y_train, X_test, y_test, w)\n",
    "AdaDelta_loss = AdaDelta_run(X_train, y_train, X_test, y_test, w)\n",
    "Adam_loss = Adam_run(X_train, y_train, X_test, y_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss图像\n",
    "x = np.arange(iter)\n",
    "\n",
    "plt.plot(x, NAG_loss.tolist(), label='NAG')\n",
    "plt.plot(x, RMSProp_loss.tolist(), label='RMSProp')\n",
    "plt.plot(x, AdaDelta_loss.tolist(), label='AdaDelta')\n",
    "plt.plot(x, Adam_loss.tolist(), label='Adam')\n",
    "\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.title('Iteration-Loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
