{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_svmlight_file(\"a9a.txt\")\n",
    "X_train = data[0].todense()\n",
    "y_train = data[1]\n",
    "data = load_svmlight_file(\"a9a.t\")\n",
    "X_test = data[0].todense()\n",
    "y_test = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.c_[X_test, np.zeros(X_test.shape[0])]      #验证集少了一个属性 补全为0\n",
    "X_train = np.c_[X_train, np.ones(X_train.shape[0])]    #加一维 全为1\n",
    "X_test = np.c_[X_test, np.ones(X_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 4000                                #迭代次数\n",
    "dimension = X_train.shape[1]              #维数\n",
    "regular_param = 1                         #正则化参数\n",
    "batch_size = int(X_train.shape[0] / 325)      #batch大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#w = np.zeros(dimension)                      #全零初始化\n",
    "w = np.random.normal(size = (dimension))      #正态分布初始化\n",
    "G = np.zeros(dimension)                       #梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X_train, y_train):\n",
    "    #打乱矩阵 取batch\n",
    "    random_sequence = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(random_sequence)\n",
    "    X_batch = np.zeros((batch_size, dimension))\n",
    "    y_batch = np.zeros(batch_size)\n",
    "    for i in range(batch_size):\n",
    "        X_batch[i] = X_train[random_sequence[i], :]\n",
    "        y_batch[i] = y_train[random_sequence[i]]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(X_train, y_train, w):\n",
    "    X_batch, y_batch = get_batch(X_train, y_train)\n",
    "    \n",
    "    predict = np.dot(X_batch, w.reshape(w.shape[0], 1))   #预测值向量(列向量)\n",
    "    g_1 = regular_param * w               #训练集梯度第一部分\n",
    "    g_2 = np.zeros(dimension)             #训练集梯度第二部分\n",
    "    for i in range(batch_size):\n",
    "        g_2 = g_2 + (y_batch[i] / (1 + np.exp(y_batch[i] * predict[i]))) * X_batch[i, :]\n",
    "    g_2 = -(1 / batch_size) * g_2\n",
    "    g = g_1 + g_2\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(X_test, y_test, w):\n",
    "    loss_1 = 0.5 * regular_param * np.linalg.norm(w, 2)\n",
    "    loss_2 = 0\n",
    "    for i in range(X_test.shape[0]):\n",
    "        loss_2 = loss_2 + np.log(1 + np.exp(-y_test[i] * np.dot(X_test[i], w.reshape(w.shape[0], 1))))\n",
    "    loss_2 = loss_2 / X_test.shape[0]\n",
    "    loss = loss_1 + loss_2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NAG(w, v, g, eta=0.05, gama=0.9):\n",
    "    v = gama * v + eta * g\n",
    "    w = w - v\n",
    "    return w, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSProp(w, G, g, gama=0.9, eta=0.001, epsilon=1e-8):\n",
    "    G = gama * G + (1 - gama) * np.dot(g, g.reshape(g.shape[0], 1))\n",
    "    w = w - (eta / np.sqrt(G + epsilon)) * g\n",
    "    return w, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AdaDelta(w, G, g, delta, gama=0.95, epsilon=1e-6):\n",
    "    G = gama * G + (1-gama) * np.dot(g, g)\n",
    "    delta_w = -(np.sqrt(delta + epsilon) / np.sqrt(G + epsilon)) * g\n",
    "    w = w + delta_w\n",
    "    delta = gama * delta + (1-gama) * np.dot(delta_w, delta_w.reshape(delta_w.shape[0], 1))\n",
    "    return w, G, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Adam(w, m, G, g, t, beta=0.9, gama=0.999, eta=0.002, epsilon=1e-8):\n",
    "    m = beta * m + (1-beta) * g\n",
    "    G = gama * G + (1-gama) * np.dot(g, g.reshape(g.shape[0], 1))\n",
    "    a = eta * np.sqrt(1 - np.power(gama, t))\n",
    "    w = w - (a / np.sqrt(G + epsilon)) * m\n",
    "    t = t + 1\n",
    "    return w, m, G, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "'''\n",
    "def logistic_regression (X_train, y_train, X_test, y_test, w):\n",
    "    #X_batch, y_batch = get_batch(X_train, y_train)\n",
    "    w_save = w\n",
    "    \n",
    "    #NAG\n",
    "    gama = 0.9\n",
    "    w = w_save\n",
    "    v = np.zeros(dimension)\n",
    "    NAG_loss = np.zeros(iter)   #初始化 存放NAG法的验证集loss的向量\n",
    "    for i in range(iter):           #迭代\n",
    "        NAG_loss[i] = loss(X_test, y_test, w)\n",
    "        g = gradient(X_train, y_train, w - gama * v)\n",
    "        w, v = NAG(w, v, g)\n",
    "     \n",
    "    #RMSProp\n",
    "    w = w_save\n",
    "    G = 0\n",
    "    RMSProp_loss = np.zeros(iter)   #初始化 存放NAG法的验证集loss的向量\n",
    "    for i in range(iter):\n",
    "        RMSProp_loss[i] = loss(X_test, y_test, w)\n",
    "        g = gradient(X_train, y_train, w)\n",
    "        w, G = RMSProp(w ,G, g)\n",
    "    \n",
    "    #AdaDelta\n",
    "    w = w_save\n",
    "    G = 0\n",
    "    delta = 0\n",
    "    AdaDelta_loss = np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        AdaDelta_loss[i] = loss(X_test, y_test, w)\n",
    "        g = gradient(X_train, y_train, w)\n",
    "        w, G, delta = AdaDelta(w, G, g, delta)\n",
    "    \n",
    "    #Adam\n",
    "    w = w_save\n",
    "    m = np.zeros(dimension)\n",
    "    G = 0\n",
    "    t = 0\n",
    "    Adam_loss = np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        Adam_loss[i] = loss(X_test, y_test, w)\n",
    "        g = gradient(X_train, y_train, w)\n",
    "        w, m, G, t = Adam(w, m, G, g, t)\n",
    "    \n",
    "    return NAG_loss, RMSProp_loss, AdaDelta_loss, Adam_loss\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAG_loss, RMSProp_loss, AdaDelta_loss, Adam_loss = logistic_regression (X_train, y_train, X_test, y_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAG_run(X_train, y_train, X_test, y_test, w):\n",
    "    gama = 0.9\n",
    "    v = np.zeros(dimension)\n",
    "    NAG_loss = np.zeros(iter)   #初始化 存放NAG法的验证集loss的向量\n",
    "    for i in range(iter):           #迭代\n",
    "        NAG_loss[i] = loss(X_test, y_test, w)\n",
    "        g = gradient(X_train, y_train, w - gama * v)\n",
    "        w, v = NAG(w, v, g)\n",
    "    return NAG_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSProp_run(X_train, y_train, X_test, y_test, w):\n",
    "    G = 0\n",
    "    RMSProp_loss = np.zeros(iter)   #初始化 存放NAG法的验证集loss的向量\n",
    "    for i in range(iter):\n",
    "        RMSProp_loss[i] = loss(X_test, y_test, w)\n",
    "        g = gradient(X_train, y_train, w)\n",
    "        w, G = RMSProp(w ,G, g)\n",
    "    return RMSProp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AdaDelta_run(X_train, y_train, X_test, y_test, w):\n",
    "    G = 0\n",
    "    delta = 0\n",
    "    AdaDelta_loss = np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        AdaDelta_loss[i] = loss(X_test, y_test, w)\n",
    "        g = gradient(X_train, y_train, w)\n",
    "        w, G, delta = AdaDelta(w, G, g, delta)\n",
    "    return AdaDelta_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Adam_run(X_train, y_train, X_test, y_test, w):\n",
    "    m = np.zeros(dimension)\n",
    "    G = 0\n",
    "    t = 0\n",
    "    Adam_loss = np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        Adam_loss[i] = loss(X_test, y_test, w)\n",
    "        g = gradient(X_train, y_train, w)\n",
    "        w, m, G, t = Adam(w, m, G, g, t)\n",
    "    return Adam_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAG_loss = NAG_run(X_train, y_train, X_test, y_test, w)\n",
    "RMSProp_loss = RMSProp_run(X_train, y_train, X_test, y_test, w)\n",
    "AdaDelta_loss = AdaDelta_run(X_train, y_train, X_test, y_test, w)\n",
    "Adam_loss = Adam_run(X_train, y_train, X_test, y_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss图像\n",
    "x = np.arange(iter)\n",
    "\n",
    "plt.plot(x, NAG_loss.tolist(), label='NAG')\n",
    "plt.plot(x, RMSProp_loss.tolist(), label='RMSProp')\n",
    "plt.plot(x, AdaDelta_loss.tolist(), label='AdaDelta')\n",
    "plt.plot(x, Adam_loss.tolist(), label='Adam')\n",
    "\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.title('Iteration-Loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
